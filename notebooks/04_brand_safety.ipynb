{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brand Safety Analysis\n",
    "## Steps 4.i, 4.ii: Toxicity Detection, Weaponized Meme Monitoring, Alert Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.brand_safety.toxicity_analyzer import ToxicityAnalyzer\n",
    "from src.brand_safety.monitoring import BrandSafetyMonitor\n",
    "from src.data_collection.knowyourmeme_scraper import KnowYourMemeCollector\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Analyzed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv('../data/reddit_predictions.csv')\n",
    "print(f\"Loaded {len(reddit_df)} memes with predictions\")\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.i: Toxicity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_analyzer = ToxicityAnalyzer()\n",
    "reddit_df = toxicity_analyzer.process_dataframe(reddit_df)\n",
    "\n",
    "print(f\"Toxicity analysis complete\")\n",
    "print(f\"Total weaponized memes: {reddit_df['is_weaponized'].sum()}\")\n",
    "print(f\"Average toxicity score: {reddit_df['toxicity_score'].mean():.2f}\")\n",
    "\n",
    "reddit_df[['body', 'toxicity_score', 'is_weaponized']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Brand Safety Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alerts = toxicity_analyzer.generate_alerts(reddit_df)\n",
    "\n",
    "print(f\"\\nBrand safety alerts generated: {len(alerts)}\")\n",
    "if len(alerts) > 0:\n",
    "    print(f\"Alert severity breakdown:\")\n",
    "    print(alerts['alert_severity'].value_counts())\n",
    "    alerts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit Safety Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_safety = toxicity_analyzer.subreddit_safety_analysis(reddit_df)\n",
    "\n",
    "print(f\"\\nSubreddit Safety Analysis:\")\n",
    "print(f\"Total subreddits analyzed: {len(subreddit_safety)}\")\n",
    "print(f\"\\nTop 10 Riskiest Subreddits:\")\n",
    "subreddit_safety.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.ii: Setup Brand Safety Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = BrandSafetyMonitor()\n",
    "monitoring_alerts = monitor.setup_monitoring(reddit_df)\n",
    "\n",
    "print(f\"Monitoring alerts configured: {len(monitoring_alerts)}\")\n",
    "if len(monitoring_alerts) > 0:\n",
    "    print(f\"\\nCritical Alerts:\")\n",
    "    monitoring_alerts[['alert_id', 'alert_type', 'ici_score', 'toxicity_score', 'recommended_action']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.i: KnowYourMeme Weaponized Pattern Cross-Reference"
   ]
  },
  {
   "cell_type": "code","execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kym_collector = KnowYourMemeCollector()\n",
    "kym_patterns = kym_collector.fetch_weaponized_patterns()\n",
    "\n",
    "weaponized_matches = []\n",
    "for idx, row in reddit_df.iterrows():\n",
    "    text_lower = str(row['body']).lower()\n",
    "    for _, pattern in kym_patterns.iterrows():\n",
    "        if pattern['meme_name'] in text_lower:\n",
    "            weaponized_matches.append({\n",
    "                'meme_id': f\"MEME_{idx}\",\n",
    "                'text': row['body'][:200],\n",
    "                'matched_pattern': pattern['meme_name'],\n",
    "                'toxicity_level': pattern['toxicity_level'],\n",
    "                'financial_risk': pattern['financial_risk']\n",
    "            })\n",
    "\n",
    "weaponized_df = pd.DataFrame(weaponized_matches)\n",
    "print(f\"\\nWeaponized pattern matches: {len(weaponized_df)}\")\n",
    "if len(weaponized_df) > 0:\n",
    "    weaponized_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Brand Safety Data to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from config.settings import PROJECT_ID, DATASET_ID\n",
    "\n",
    "if len(monitoring_alerts) > 0:\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.brand_safety_alerts\"\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\")\n",
    "    job = client.load_table_from_dataframe(monitoring_alerts, table_id, job_config=job_config)\n",
    "    job.result()\n",
    "    \n",
    "    print(f\"Brand safety alerts exported to {table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand Safety Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Brand Safety Analysis Summary:\")\n",
    "print(f\"Total memes analyzed: {len(reddit_df)}\")\n",
    "print(f\"\\nToxicity Metrics:\")\n",
    "print(f\"  Weaponized memes: {reddit_df['is_weaponized'].sum()}\")\n",
    "print(f\"  Average toxicity score: {reddit_df['toxicity_score'].mean():.2f}\")\n",
    "print(f\"  High toxicity (>3): {(reddit_df['toxicity_score'] >= 3).sum()}\")\n",
    "print(f\"\\nAlerts Generated:\")\n",
    "print(f\"  Total alerts: {len(monitoring_alerts)}\")\n",
    "print(f\"  KnowYourMeme matches: {len(weaponized_df)}\")\n",
    "print(f\"\\nRiskiest Subreddit: {subreddit_safety.iloc[0]['subreddit']}\")\n",
    "print(f\"  Weaponized rate: {subreddit_safety.iloc[0]['weaponized_rate']*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
