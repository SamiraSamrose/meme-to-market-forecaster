{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Analysis with Vertex AI\n",
    "## Steps 1.i, 1.ii, 2.i: Semantic Scoring, Vector Clustering, Visual Metaphor Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.semantic_analysis.semantic_scoring import SemanticScorer\n",
    "from src.semantic_analysis.vertex_ai_gemini import GeminiAnalyzer\n",
    "from src.semantic_analysis.vector_clustering import VectorClusterer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.i: Calculate Meme Seriousness Threshold and Irony Collapse Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv('../data/reddit_meme_raw.csv')\n",
    "\n",
    "scorer = SemanticScorer()\n",
    "semantic_results = scorer.process_batch(reddit_df['body'].tolist()[:1000])\n",
    "\n",
    "reddit_df_analyzed = reddit_df.head(len(semantic_results)).copy()\n",
    "reddit_df_analyzed = reddit_df_analyzed.join(semantic_results)\n",
    "\n",
    "print(f\"Semantic scores calculated for {len(semantic_results)} memes\")\n",
    "print(f\"Average Seriousness Threshold: {semantic_results['seriousness_threshold'].mean():.3f}\")\n",
    "print(f\"Average ICI: {semantic_results['irony_collapse_index'].mean():.3f}\")\n",
    "\n",
    "semantic_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.i: Save Semantic Scores to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from config.settings import PROJECT_ID, DATASET_ID\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "semantic_batch = []\n",
    "for idx, row in reddit_df_analyzed.head(500).iterrows():\n",
    "    semantic_batch.append({\n",
    "        'meme_id': f\"MEME_{idx}\",\n",
    "        'text': row['body'][:500],\n",
    "        'meme_seriousness_threshold': row['seriousness_threshold'],\n",
    "        'irony_collapse_index': row['irony_collapse_index'],\n",
    "        'financial_keywords_count': row['financial_keywords_count'],\n",
    "        'humor_keywords_count': row['humor_keywords_count']\n",
    "    })\n",
    "\n",
    "semantic_df = pd.DataFrame(semantic_batch)\n",
    "table_id = f\"{PROJECT_ID}.{DATASET_ID}.semantic_scores\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\")\n",
    "job = client.load_table_from_dataframe(semantic_df, table_id, job_config=job_config)\n",
    "job.result()\n",
    "\n",
    "print(f\"Semantic scores saved to {table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.ii: Vector Clustering Against Historical Market Movers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = VectorClusterer()\n",
    "reddit_df_analyzed = clusterer.find_lookalikes(reddit_df_analyzed)\n",
    "reddit_df_analyzed = clusterer.perform_clustering(reddit_df_analyzed, n_clusters=6)\n",
    "\n",
    "print(f\"Vector clustering complete\")\n",
    "print(f\"Average lookalike similarity: {reddit_df_analyzed['lookalike_similarity'].mean():.3f}\")\n",
    "print(f\"High similarity memes (>0.7): {(reddit_df_analyzed['lookalike_similarity'] > 0.7).sum()}\")\n",
    "\n",
    "reddit_df_analyzed[['body', 'best_lookalike', 'lookalike_similarity', 'cluster']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.i: Visual Metaphor Analysis with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = GeminiAnalyzer()\n",
    "\n",
    "visual_results = []\n",
    "for text in reddit_df_analyzed['body'].head(100):\n",
    "    analysis = gemini.analyze_visual_metaphor(text)\n",
    "    visual_results.append({\n",
    "        'primary_metaphor': analysis['primary_visual_metaphor'],\n",
    "        'shopping_correlation': analysis['google_shopping_correlation'],\n",
    "        'metaphor_count': analysis['metaphor_count']\n",
    "    })\n",
    "\n",
    "visual_df = pd.DataFrame(visual_results)\n",
    "reddit_df_analyzed = reddit_df_analyzed.head(len(visual_df)).join(visual_df)\n",
    "\n",
    "print(f\"Visual metaphor analysis complete\")\n",
    "print(f\"Memes with visual metaphors: {(visual_df['metaphor_count'] > 0).sum()}\")\n",
    "print(f\"Average shopping correlation: {visual_df['shopping_correlation'].mean():.3f}\")\n",
    "\n",
    "visual_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.ii: Export Visual Metadata to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.storage import StorageManager\n",
    "\n",
    "storage_manager = StorageManager()\n",
    "\n",
    "visual_metadata = []\n",
    "for idx, row in reddit_df_analyzed.iterrows():\n",
    "    visual_metadata.append({\n",
    "        'meme_id': f\"MEME_{idx}\",\n",
    "        'primary_visual_metaphor': row.get('primary_metaphor', 'none'),\n",
    "        'shopping_correlation_score': float(row.get('shopping_correlation', 0.0)),\n",
    "        'metaphor_count': int(row.get('metaphor_count', 0)),\n",
    "        'seriousness_threshold': float(row['seriousness_threshold']),\n",
    "        'irony_collapse_index': float(row['irony_collapse_index'])\n",
    "    })\n",
    "\n",
    "storage_manager.export_visual_metadata(visual_metadata)\n",
    "print(\"Visual metadata exported to GCS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Semantic Analysis Summary:\")\n",
    "print(f\"Total memes analyzed: {len(reddit_df_analyzed)}\")\n",
    "print(f\"\\nSemantic Scores:\")\n",
    "print(f\"  Avg Seriousness Threshold: {reddit_df_analyzed['seriousness_threshold'].mean():.3f}\")\n",
    "print(f\"  Avg ICI: {reddit_df_analyzed['irony_collapse_index'].mean():.3f}\")\n",
    "print(f\"\\nClustering:\")\n",
    "print(f\"  Unique clusters: {reddit_df_analyzed['cluster'].nunique()}\")\n",
    "print(f\"  Avg lookalike similarity: {reddit_df_analyzed['lookalike_similarity'].mean():.3f}\")\n",
    "print(f\"\\nVisual Metaphors:\")\n",
    "print(f\"  Memes with metaphors: {(reddit_df_analyzed['metaphor_count'] > 0).sum()}\")\n",
    "print(f\"  Avg shopping correlation: {reddit_df_analyzed['shopping_correlation'].mean():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
